{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rraj/PythonFunctions/DCNet'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/rraj/PythonFunctions/DCNet/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData:\n",
    "    def read_data(self, data_identifier: str, data_type: str):\n",
    "        file_path = self.check_identifier(data_identifier)\n",
    "        \n",
    "        f = shelve.open(file_path[:-3], 'r')\n",
    "        if data_type == 'train':\n",
    "            data = f['train_dict']\n",
    "        elif data_type == 'test':\n",
    "            data = f['train_dict']\n",
    "        else:\n",
    "            raise Exception(\"invalid data type requested\")\n",
    "        f.close()\n",
    "        return data\n",
    "    \n",
    "    def check_identifier(self, data_identifier: str):\n",
    "        file_path = data_identifier\n",
    "        if 'Data' not in data_identifier:\n",
    "            file_path = os.path.join(os.getcwd(), 'Data', 'Data-'+data_identifier+'.db')\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"{file_path} not found\")\n",
    "        else:\n",
    "            return file_path\n",
    "        \n",
    "        \n",
    "    def labels_to_index_dict(self, data: dict):\n",
    "        return {label : indx for indx, label in enumerate(data.keys())}\n",
    "    \n",
    "    def index_to_labels_dict(self, labels_to_index_dict: dict):\n",
    "        return {indx : label for label, indx in labels_to_index_dict.items()}\n",
    "    \n",
    "    def get_nclasses(self, data: dict):\n",
    "        return len(data.keys())\n",
    "    \n",
    "    def get_sample_sizes(self, data: dict, index_to_label_dict: dict):\n",
    "        sample_sizes = []\n",
    "        for indx in range(len(data)):\n",
    "            label = index_to_label_dict.get(indx, None)\n",
    "            sample_sizes.append(data[label].shape[1])\n",
    "        return sample_sizes\n",
    "\n",
    "    def get_max_batch_size(self, sample_sizes: list):\n",
    "        return min(sample_sizes)*len(sample_sizes)\n",
    "    \n",
    "    def get_max_data_dim(self, data: dict):\n",
    "        data_dims = [values.shape[0] for values in data.values()]\n",
    "        return max(data_dims)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(ModelData, Dataset):\n",
    "    def __init__(self, data_identifier: str):\n",
    "        self.data = self.read_data(data_identifier, 'train')\n",
    "        self.nclasses = self.get_nclasses(self.data)\n",
    "        self.labels_to_index = self.labels_to_index_dict(self.data)\n",
    "        self.index_to_labels = self.index_to_labels_dict(self.labels_to_index)\n",
    "        self.sample_sizes = self.get_sample_sizes(self.data, self.index_to_labels)\n",
    "        self.max_batch_size = self.get_max_batch_size(self.sample_sizes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        total_samples = 0\n",
    "        for value in self.data.values():\n",
    "            total_samples += value.shape[1]\n",
    "        return total_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, tuple) and len(index) == 2:\n",
    "            k = self.index_to_labels.get(index[0], None)\n",
    "            return torch.unsqueeze(torch.from_numpy(self.data[k][:, index[1]]), 0)\n",
    "        else:\n",
    "            raise IndexError(f\"{index} not supported\")\n",
    "        \n",
    "    def get_data_dim(self):\n",
    "        return self.get_max_data_dim(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(ModelData, Dataset):\n",
    "    def __init__(self, data_identifier: str):\n",
    "        self.data = self.read_data(data_identifier, 'test')\n",
    "        self.labels_to_index = self.labels_to_index_dict(self.data)\n",
    "        self.nclasses = self.get_nclasses(self.data)\n",
    "        self.index_to_labels = self.index_to_labels_dict(self.labels_to_index)\n",
    "        self.sample_sizes = self.get_sample_sizes(self.data, self.index_to_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_samples = 0\n",
    "        for value in self.data.values():\n",
    "            total_samples += value.shape[1]\n",
    "        return total_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, tuple) and len(index) == 2:\n",
    "            k = self.index_to_labels.get(index[0], None)\n",
    "            return torch.unsqueeze(torch.from_numpy(self.data[k][:, index[1]]), 0)\n",
    "        else:\n",
    "            raise IndexError(f\"{index} not supported\")\n",
    "        \n",
    "    def get_data_dim(self):\n",
    "        return self.get_max_data_dim(self.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoader:\n",
    "    def __init__(self, data_source, batch_size: int = 0, n_iter: int = 1, shuffle: bool = False):\n",
    "        self.data_source = data_source\n",
    "        self.n_iter = n_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.returned_index = 0\n",
    "        self.adjusted_batch_size = self.adjust_batch_size(batch_size)\n",
    "        self.nsamples = self.get_nsamples()\n",
    "        self.class_indices_r = self.get_randomized_class_indices()\n",
    "        self.sample_indices_r = self.get_randomized_sample_indices()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n_iter > 0 and self.returned_index < self.adjusted_batch_size:\n",
    "            c_indx, s_indx = divmod(self.returned_index, self.nsamples)\n",
    "            self.returned_index += 1\n",
    "            self.update_batch()\n",
    "            if self.shuffle:\n",
    "                indx = self.class_indices_r[c_indx], random.randrange(self.data_source.sample_sizes[c_indx])\n",
    "                return self.data_source[indx]\n",
    "            else:\n",
    "                indx = self.class_indices_r[c_indx], (self.sample_indices_r[c_indx] + s_indx)\n",
    "                return self.data_source[indx]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def update_batch(self):\n",
    "        if self.returned_index == self.adjusted_batch_size:\n",
    "            self.n_iter -= 1\n",
    "            random.shuffle(self.class_indices_r)\n",
    "            self.sample_indices_r = self.get_randomized_sample_indices()\n",
    "            self.returned_index = 0\n",
    "        \n",
    "    def adjust_batch_size(self, batch_size: int):\n",
    "        adjusted_batch_size = min(max(batch_size//self.data_source.nclasses, 1) * self.data_source.nclasses, self.data_source.max_batch_size)\n",
    "        if adjusted_batch_size != batch_size:\n",
    "            warnings.warn(f\"batch size adjusted to {adjusted_batch_size}\")\n",
    "        return adjusted_batch_size\n",
    "\n",
    "    def get_nsamples(self):\n",
    "        return (self.adjusted_batch_size//self.data_source.nclasses)\n",
    "    \n",
    "    def get_randomized_class_indices(self):\n",
    "        class_indices_r = [*range(self.data_source.nclasses)]\n",
    "        random.shuffle(class_indices_r)\n",
    "        return class_indices_r\n",
    "    \n",
    "    def get_randomized_sample_indices(self):\n",
    "        sample_indices_r = []\n",
    "        for indx in self.class_indices_r:\n",
    "            random_start_limit = self.data_source.sample_sizes[indx] - self.nsamples\n",
    "            sample_indices_r.append(random.randint(0, random_start_limit))\n",
    "        return sample_indices_r\n",
    "    \n",
    "    def get_batch_size(self):\n",
    "        return self.adjusted_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLoader:\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        self.returned_class_indx = 0\n",
    "        self.returned_sample_indx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.returned_class_indx < self.data_source.nclasses and self.returned_sample_indx < self.data_source.sample_sizes[self.returned_class_indx]:\n",
    "            indx = self.returned_class_indx, self.returned_sample_indx\n",
    "            self.returned_sample_indx += 1\n",
    "            self.update_class()\n",
    "            return self.data_source[indx]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def update_class(self):\n",
    "        if self.returned_sample_indx == self.data_source.sample_sizes[self.returned_class_indx]:\n",
    "            self.returned_class_indx += 1\n",
    "            self.returned_sample_indx = 0\n",
    "\n",
    "    def get_index_to_class_dict(self):\n",
    "        return self.data_source.index_to_labels\n",
    "    \n",
    "    def get_class_to_index_dict(self):\n",
    "        return self.data_source.labels_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = TrainData('May04-2357')\n",
    "train_loader = TrainLoader(train_inputs, batch_size=128, n_iter=2)\n",
    "test_inputs = TestData('May04-2357')\n",
    "test_loader = TestLoader(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:This message should appear on the console\n",
      "INFO:So should this\n",
      "WARNING:And this, too\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "logging.debug('This message should appear on the console')\n",
    "logging.info('So should this')\n",
    "logging.warning('And this, too')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from torch.linalg import multi_dot\n",
    "\n",
    "def get_init_batch(train_inputs: TrainData, n_batches: int=1):\n",
    "    data_dim = train_inputs.get_data_dim()\n",
    "    batch_size = int(0.25*len(train_inputs))\n",
    "    train_loader = TrainLoader(train_inputs, batch_size=batch_size, n_iter=n_batches)\n",
    "    adjusted_batch_size = train_loader.get_batch_size()\n",
    "    init_tensor = torch.empty((n_batches, adjusted_batch_size, data_dim))\n",
    "    for i, input in enumerate(train_loader):\n",
    "        batch_number, sample_number = divmod(i, adjusted_batch_size)\n",
    "        init_tensor[batch_number, sample_number, :] = torch.flatten(input, start_dim=0)\n",
    "    return init_tensor\n",
    "\n",
    "def get_ncomps(S: torch.Tensor):\n",
    "    assert S.dim() == 1\n",
    "    S_n = S**2/torch.sum(S**2)\n",
    "    n_comp = 0\n",
    "    sum_comp = 0\n",
    "    for comp in S_n:\n",
    "        sum_comp += comp\n",
    "        n_comp += 1\n",
    "        if sum_comp > 0.95:\n",
    "            break\n",
    "    return n_comp\n",
    "\n",
    "def randomizer_matrix(m: int, n: int):\n",
    "    assert m > n\n",
    "    temp = torch.rand(m, m)\n",
    "    U, _, _ = torch.pca_lowrank(temp)\n",
    "    return U[:, :n]\n",
    "\n",
    "def input_svd_matrices(init_batch: torch.Tensor):\n",
    "    _, S, V = torch.pca_lowrank(init_batch)\n",
    "    n_comps = get_ncomps(S)\n",
    "    return V[:, :n_comps], S[:n_comps], n_comps\n",
    "\n",
    "def initialize_network_connections(layer_dims: list, data_identifier: str):\n",
    "    train_inputs = TrainData(data_identifier)\n",
    "    init_batch = get_init_batch(train_inputs)\n",
    "    left_matrix, sigma, n = input_svd_matrices(init_batch)\n",
    "    sigma_r = sigma**(-1/len(layer_dims))\n",
    "    \n",
    "    for dim in layer_dims:\n",
    "        right_matrix = randomizer_matrix(layer_dims[dim], n)\n",
    "        w = multi_dot([left_matrix, torch.diag(sigma_r), right_matrix.T])\n",
    "        w_n = normalize(w, p=2.0, dim=1)\n",
    "        left_matrix = right_matrix\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/8km7dn7s35bfc4y81hgxnfj9d52_hx/T/ipykernel_43991/2289762838.py:39: UserWarning: batch size adjusted to 64\n",
      "  warnings.warn(f\"batch size adjusted to {adjusted_batch_size}\")\n"
     ]
    }
   ],
   "source": [
    "train_inputs = TrainData('May04-2357')\n",
    "init_tensor = get_init_batch(train_inputs, n_batches=3)\n",
    "print(init_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Hash(nn.Module):\n",
    "    def __init__(self, input_size: int, hash_length: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, hash_length, bias=False)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 3\n",
    "        out = self.fc(x)\n",
    "        out = self.activation(out)\n",
    "        return self.hash_function(torch.sign(out))\n",
    "    \n",
    "    def hash_function(self, out: torch.Tensor):\n",
    "        out_f = torch.flatten(out, end_dim=1)\n",
    "        hash_values = torch.empty((out_f.shape[0], 1))\n",
    "        for indx, row in enumerate(out_f):\n",
    "            hash_values[indx, 0] = sum(v*2**i for i, v in enumerate(reversed(row)))\n",
    "        return hash_values.reshape(*out.shape[:-1], 1)\n",
    "\n",
    "\n",
    "class MultiHash(nn.Module):\n",
    "    def __init__(self, input_size: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.input_size = input_size\n",
    "        self.hash_length = 2*input_size #find optimal number of partitions\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        hash_tensor = torch.empty((*x.shape[:-1], self.n_heads))\n",
    "        for head in range(self.n_heads):\n",
    "            hash_layer = Hash(self.input_size, self.hash_length)\n",
    "            hash_tensor[:, :, head] = torch.flatten(hash_layer(x), start_dim=1)\n",
    "        return hash_tensor\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# implement to get input indices or make sure that at max bach size inputs are generated orederly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------input tensor shapes-----\n",
      "torch.Size([1, 1, 4])\n",
      "torch.Size([1, 5, 4])\n",
      "torch.Size([2, 5, 4])\n",
      "------output tensor shapes-----\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 5, 1])\n",
      "torch.Size([2, 5, 1])\n",
      "------hashpool tensor shapes-----\n",
      "torch.Size([1, 1, 3])\n",
      "torch.Size([1, 5, 3])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor([[[1, 2, 3, 4]]], dtype=torch.float)\n",
    "a2 = torch.tensor([[[1, 1, 1, 1],[5, 6, 7, 8], [0, 0, 0, 0], [6, 4, 5, 2], [7, 3, 9, 1]]], dtype=torch.float)\n",
    "a3 = torch.vstack((a2, a2)).reshape(2, 5, 4)\n",
    "\n",
    "print('------input tensor shapes-----')\n",
    "print(a1.shape)\n",
    "print(a2.shape)\n",
    "print(a3.shape)\n",
    "\n",
    "hash_layer = Hash(4, 3)\n",
    "\n",
    "print('------output tensor shapes-----')\n",
    "print(hash_layer(a1).shape)\n",
    "print(hash_layer(a2).shape)\n",
    "print(hash_layer(a3).shape)\n",
    "\n",
    "hash_pool = MultiHash(4, 3)\n",
    "\n",
    "print('------hashpool tensor shapes-----')\n",
    "print(hash_pool(a1).shape)\n",
    "print(hash_pool(a2).shape)\n",
    "print(hash_pool(a3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[180., 118.,  49.]]], grad_fn=<CopySlices>)\n",
      "------**------\n",
      "tensor([[[203.,  73., 212.],\n",
      "         [203., 129., 214.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [203., 217., 179.],\n",
      "         [203., 155., 215.]]], grad_fn=<CopySlices>)\n",
      "------**------\n",
      "tensor([[[ 97., 192., 124.],\n",
      "         [ 97., 192., 124.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [101., 130., 220.],\n",
      "         [100., 130., 216.]],\n",
      "\n",
      "        [[ 97., 192., 124.],\n",
      "         [ 97., 192., 124.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [101., 130., 220.],\n",
      "         [100., 130., 216.]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(hash_pool(a1))\n",
    "print(\"------**------\")\n",
    "print(hash_pool(a2))\n",
    "print(\"------**------\")\n",
    "print(hash_pool(a3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor([[1, 2, 3]])\n",
    "torch.flatten(inp, start_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2   3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
