{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rraj/PythonFunctions/DCNet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/rraj/PythonFunctions/DCNet/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData:\n",
    "    def read_data(self, data_identifier: str, data_type: str):\n",
    "        file_path = self.check_identifier(data_identifier)\n",
    "        \n",
    "        f = shelve.open(file_path[:-3], 'r')\n",
    "        if data_type == 'train':\n",
    "            data = f['train_dict']\n",
    "        elif data_type == 'test':\n",
    "            data = f['train_dict']\n",
    "        else:\n",
    "            raise Exception(\"invalid data type requested\")\n",
    "        f.close()\n",
    "        return data\n",
    "    \n",
    "    def check_identifier(self, data_identifier: str):\n",
    "        file_path = data_identifier\n",
    "        if 'Data' not in data_identifier:\n",
    "            file_path = os.path.join(os.getcwd(), 'Data', 'Data-'+data_identifier+'.db')\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"{file_path} not found\")\n",
    "        else:\n",
    "            return file_path\n",
    "        \n",
    "        \n",
    "    def labels_to_index_dict(self, data: dict):\n",
    "        return {label : indx for indx, label in enumerate(data.keys())}\n",
    "    \n",
    "    def index_to_labels_dict(self, labels_to_index_dict: dict):\n",
    "        return {indx : label for label, indx in labels_to_index_dict.items()}\n",
    "    \n",
    "    def get_nclasses(self, data: dict):\n",
    "        return len(data.keys())\n",
    "    \n",
    "    def get_sample_sizes(self, data: dict, index_to_label_dict: dict):\n",
    "        sample_sizes = []\n",
    "        for indx in range(len(data)):\n",
    "            label = index_to_label_dict.get(indx, None)\n",
    "            sample_sizes.append(data[label].shape[1])\n",
    "        return sample_sizes\n",
    "\n",
    "    def get_max_batch_size(self, sample_sizes: list):\n",
    "        return min(sample_sizes)*len(sample_sizes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(ModelData, Dataset):\n",
    "    def __init__(self, data_identifier: str):\n",
    "        self.data = self.read_data(data_identifier, 'train')\n",
    "        self.nclasses = self.get_nclasses(self.data)\n",
    "        self.labels_to_index = self.labels_to_index_dict(self.data)\n",
    "        self.index_to_labels = self.index_to_labels_dict(self.labels_to_index)\n",
    "        self.sample_sizes = self.get_sample_sizes(self.data, self.index_to_labels)\n",
    "        self.max_batch_size = self.get_max_batch_size(self.sample_sizes)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_samples = 0\n",
    "        for value in self.data.values():\n",
    "            total_samples += value.shape[1]\n",
    "        return total_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, tuple) and len(index) == 2:\n",
    "            k = self.index_to_labels.get(index[0], None)\n",
    "            return torch.from_numpy(self.data[k][:, index[1]:index[1]+1])\n",
    "        else:\n",
    "            raise IndexError(f\"{index} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(ModelData, Dataset):\n",
    "    def __init__(self, data_identifier: str):\n",
    "        self.data = self.read_data(data_identifier, 'test')\n",
    "        self.labels_to_index = self.labels_to_index_dict(self.data)\n",
    "        self.nclasses = self.get_nclasses(self.data)\n",
    "        self.index_to_labels = self.index_to_labels_dict(self.labels_to_index)\n",
    "        self.sample_sizes = self.get_sample_sizes(self.data, self.index_to_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        total_samples = 0\n",
    "        for value in self.data.values():\n",
    "            total_samples += value.shape[1]\n",
    "        return total_samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, tuple) and len(index) == 2:\n",
    "            k = self.index_to_labels.get(index[0], None)\n",
    "            return torch.from_numpy(self.data[k][:, index[1]:index[1]+1])\n",
    "        else:\n",
    "            raise IndexError(f\"{index} not supported\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoader:\n",
    "    def __init__(self, data_source, batch_size: int = 0, n_iter: int = 1, shuffle: bool = False):\n",
    "        self.data_source = data_source\n",
    "        self.n_iter = n_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.returned_index = 0\n",
    "        self.adjusted_batch_size = self.adjust_batch_size(batch_size)\n",
    "        self.nsamples = self.get_nsamples()\n",
    "        self.class_indices_r = self.get_randomized_class_indices()\n",
    "        self.sample_indices_r = self.get_randomized_sample_indices()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n_iter > 0 and self.returned_index < self.adjusted_batch_size:\n",
    "            c_indx, s_indx = divmod(self.returned_index, self.nsamples)\n",
    "            self.returned_index += 1\n",
    "            self.update_batch()\n",
    "            if self.shuffle:\n",
    "                indx = self.class_indices_r[c_indx], random.randrange(self.data_source.sample_sizes[c_indx])\n",
    "                return self.data_source[indx]\n",
    "            else:\n",
    "                indx = self.class_indices_r[c_indx], (self.sample_indices_r[c_indx] + s_indx)\n",
    "                return self.data_source[indx]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def update_batch(self):\n",
    "        if self.returned_index == self.adjusted_batch_size:\n",
    "            self.n_iter -= 1\n",
    "            random.shuffle(self.class_indices_r)\n",
    "            self.sample_indices_r = self.get_randomized_sample_indices()\n",
    "            self.returned_index = 0\n",
    "        \n",
    "    def adjust_batch_size(self, batch_size: int):\n",
    "        adjusted_batch_size = min(max(batch_size//self.data_source.nclasses, 1) * self.data_source.nclasses, self.data_source.max_batch_size)\n",
    "        if adjusted_batch_size != batch_size:\n",
    "            warnings.warn(f\"batch size adjusted to {adjusted_batch_size}\")\n",
    "        return adjusted_batch_size\n",
    "\n",
    "    def get_nsamples(self):\n",
    "        return (self.adjusted_batch_size//self.data_source.nclasses)\n",
    "    \n",
    "    def get_randomized_class_indices(self):\n",
    "        class_indices_r = [*range(self.data_source.nclasses)]\n",
    "        random.shuffle(class_indices_r)\n",
    "        return class_indices_r\n",
    "    \n",
    "    def get_randomized_sample_indices(self):\n",
    "        sample_indices_r = []\n",
    "        for indx in self.class_indices_r:\n",
    "            random_start_limit = self.data_source.sample_sizes[indx] - self.nsamples\n",
    "            sample_indices_r.append(random.randint(0, random_start_limit))\n",
    "        return sample_indices_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLoader:\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        self.returned_class_indx = 0\n",
    "        self.returned_sample_indx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.returned_class_indx < self.data_source.nclasses and self.returned_sample_indx < self.data_source.sample_sizes[self.returned_class_indx]:\n",
    "            indx = self.returned_class_indx, self.returned_sample_indx\n",
    "            self.returned_sample_indx += 1\n",
    "            self.update_class()\n",
    "            return self.data_source[indx]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def update_class(self):\n",
    "        if self.returned_sample_indx == self.data_source.sample_sizes[self.returned_class_indx]:\n",
    "            self.returned_class_indx += 1\n",
    "            self.returned_sample_indx = 0\n",
    "\n",
    "    def get_index_to_class_dict(self):\n",
    "        return self.data_source.index_to_labels\n",
    "    \n",
    "    def get_class_to_index_dict(self):\n",
    "        return self.data_source.labels_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = TrainData('May04-2357')\n",
    "train_loader = TrainLoader(train_inputs, batch_size=128, n_iter=2)\n",
    "test_inputs = TestData('May04-2357')\n",
    "test_loader = TestLoader(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_network_weights(data_identifier: str):\n",
    "    train_inputs = TrainData(data_identifier)\n",
    "    batch_size = int(0.25*len(train_inputs))\n",
    "    train_loader = TrainLoader(train_inputs, batch_size=batch_size)\n",
    "    \n",
    "    init_batch = torch.tensor([])\n",
    "    for input in train_loader:\n",
    "        try:\n",
    "            init_batch = torch.hstack((init_batch, input))\n",
    "        except:\n",
    "            init_batch = input\n",
    "    return init_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/8km7dn7s35bfc4y81hgxnfj9d52_hx/T/ipykernel_14261/1080349714.py:39: UserWarning: batch size adjusted to 64\n",
      "  warnings.warn(f\"batch size adjusted to {adjusted_batch_size}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 64])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = initialize_network_weights('May04-2357')\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(102, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(w.numpy().T)\n",
    "total_var = 0\n",
    "number_of_comps = 0\n",
    "for comp in range(pca.n_components_):\n",
    "    total_var += pca.explained_variance_ratio_[comp]\n",
    "    if total_var > 0.95:\n",
    "        number_of_comps = comp + 1\n",
    "        break\n",
    "print(number_of_comps)\n",
    "Q_t = pca.components_[0:number_of_comps, :]\n",
    "Q = np.matmul(Q_t.T, np.diag(pca.singular_values_[:number_of_comps]))\n",
    "\n",
    "N = ortho_group.rvs(dim=500)\n",
    "phi = np.matmul(Q, N[:number_of_comps, :])\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 102)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = TrainData('May04-2357')\n",
    "train_loader = TrainLoader(train_inputs, batch_size=train_inputs.max_batch_size, n_iter=5)\n",
    "\n",
    "batch = np.array([])\n",
    "for input in train_loader:\n",
    "    try:\n",
    "        batch = np.vstack((batch, input.numpy()))\n",
    "    except:\n",
    "        batch = input.numpy()\n",
    "\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:This message should appear on the console\n",
      "INFO:So should this\n",
      "WARNING:And this, too\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "logging.debug('This message should appear on the console')\n",
    "logging.info('So should this')\n",
    "logging.warning('And this, too')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from torch.linalg import multi_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3., 2., 1.],\n",
      "        [5., 6., 7., 8.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[ 1.5000,  0.5000, -0.5000, -1.5000],\n",
      "        [-1.5000, -0.5000,  0.5000,  1.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[4.0000, 3.0000, 2.0000, 1.0000],\n",
      "        [5.0000, 6.0000, 7.0000, 8.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[4, 3, 2, 1],[5, 6, 7, 8], [0, 0, 0, 0]], dtype=torch.float)\n",
    "print(a)\n",
    "a_centered = a - torch.mean(a, dim=1, keepdim=True)\n",
    "print(a_centered)\n",
    "U, S, V = torch.pca_lowrank(a, center=False)\n",
    "x = torch.matmul(U, torch.diag(S))\n",
    "y = torch.matmul(x, V.T)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([13.9901,  2.8770,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "def get_init_batch(train_inputs: TrainData):\n",
    "    # implement with row wise operations\n",
    "    batch_size = int(0.25*len(train_inputs))\n",
    "    train_loader = TrainLoader(train_inputs, batch_size=batch_size)\n",
    "    init_batch = torch.tensor([])\n",
    "    for input in train_loader:\n",
    "        try:\n",
    "            init_batch = torch.hstack((init_batch, input))\n",
    "        except:\n",
    "            init_batch = input\n",
    "    return init_batch\n",
    "\n",
    "def get_ncomps(S: torch.Tensor):\n",
    "    S_n = S**2/torch.sum(S**2)\n",
    "    n_comp = 0\n",
    "    sum_comp = 0\n",
    "    for comp in S_n:\n",
    "        sum_comp += comp\n",
    "        n_comp += 1\n",
    "        if sum_comp > 0.95:\n",
    "            break\n",
    "    return n_comp\n",
    "\n",
    "def randomizer_matrix(m: int, n: int):\n",
    "    assert m > n\n",
    "    temp = torch.rand(m, m)\n",
    "    U, _, _ = torch.pca_lowrank(temp)\n",
    "    return U[:, :n]\n",
    "\n",
    "def input_svd_matrices(init_batch: torch.Tensor):\n",
    "    init_batch_centered =  init_batch - torch.mean(init_batch, dim=1, keepdim=True)\n",
    "    U, S, _ = torch.pca_lowrank(init_batch_centered)\n",
    "    n_comps = get_ncomps(S)\n",
    "    return U[:, :n_comps], S[:n_comps], n_comps\n",
    "\n",
    "def initialize_network_connections(layer_dims: list, data_identifier: str):\n",
    "    train_inputs = TrainData(data_identifier)\n",
    "    init_batch = get_init_batch(train_inputs)\n",
    "    right_matrix, sigma, N = input_svd_matrices(init_batch)\n",
    "    sigma_n = sigma**(-1/len(layer_dims))\n",
    "    #left_matrix = get_randomizer_matrix(layer_dims[0], n)\n",
    "    for dim in layer_dims:\n",
    "        left_matrix = randomizer_matrix(layer_dims[dim], N)\n",
    "        w = multi_dot([left_matrix, torch.diag(sigma_n), right_matrix.T])\n",
    "        w_n = normalize(w, p = 2.0, dim = 0)\n",
    "        right_matrix = left_matrix\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0715, 0.3476,    inf])\n"
     ]
    }
   ],
   "source": [
    "S_n = S**2/torch.sum(S**2)\n",
    "[torch.sum(S_n[:k]) for k in range(1, len(S_n)+1)]\n",
    "print(S**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3404, -0.9403,  0.0000],\n",
      "        [-0.9403,  0.3404,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0000]])\n",
      "tensor([[-0.3404, -0.9403,  0.0000],\n",
      "        [-0.9403,  0.3404,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 3.0000, 2.0000, 1.0000],\n",
       "        [5.0000, 6.0000, 7.0000, 8.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(U)\n",
    "print(torch.transpose(U, 1, 0))\n",
    "torch.linalg.multi_dot([U, torch.diag(S), V.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "class HashLayer(nn.Module):\n",
    "    def __init__(self, input_size: int, hash_length: int, n_hash: int):\n",
    "        super().__init__()\n",
    "        self.hl = nn.Linear(input_size, hash_length*n_hash)\n",
    "        # initialize custom wights (maybe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hl(x)\n",
    "        out = self.hash_function(out)\n",
    "        table = self.map_hash_table(out)\n",
    "        return out\n",
    "    \n",
    "    def hash_function(self, out):\n",
    "        pass\n",
    "\n",
    "    def map_hash_table(self, out):\n",
    "        # identify a logic to integrate it as network layer \n",
    "        pass\n",
    "\n",
    "# implement to get input indices or make sure that at max bach size inputs are generated orederly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
